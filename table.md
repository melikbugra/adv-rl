| **Feature**                          | **Our Method (Value + Uncertainty ARL)**                                                                                                                                | **Laux et al. 2020 (Value-Only ARL)**                                                                                                                                       | **Pinto 2017 (RARL)**                                                                                                                       | **Pan 2019 (RARARL)**                                                                                                                                                                       | **Dennis 2020 (PAIRED)**                                                                                                                                                       | **Sukhbaatar 2018 (ASP)**                                                                                                                                                                                           | **Pathak 2019 (Disagr. Exp.)**                                                                                                                                                                           | **Burda 2018 (Curiosity)**                                                                                                                                                                       | **Florensa 2018 (Goal GAN)**                                                                                                                                                                                                      | **Chen 2021 (REDQ)**                                                                                                                                                                                                                                                                    |
| ------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Adversary‚Äôs Role**                 | Places protagonist in hard **states** (maze start positions)                                                                                                            | Places protagonist in hard states (maze, disentangling setup)                                                                                                               | Injects *forces* on protagonist (perturbs dynamics)                                                                                         | Perturbs environment parameters (e.g., driving sim) over multiple steps                                                                                                                     | Generates entire environment (layout, dynamics) for a episode                                                                                                                  | Proposes tasks (sequence of actions), reversed by second agent                                                                                                                                                      | *No adversary agent* (intrinsic reward to protagonist)                                                                                                                                                   | *No adversary*, protagonist explores on its own                                                                                                                                                  | Proposes goal states via GAN (for goal-conditioned agent)                                                                                                                                                                         | *No adversary*, single-agent algorithm                                                                                                                                                                                                                                                  |
| **Training Framework**               | Two SAC agents, alternate phases                                                                                                                                        | Two SAC agents, alternate phases                                                                                                                                            | Two agents (actor-critic), simultaneous or alternating (zero-sum game)                                                                      | Two agents (DQN or DDPG variants), alternating; risk-sensitive updates                                                                                                                      | Three agents: Teacher (env gen), Protagonist, Antagonist ‚Äì minimax-regret training                                                                                             | Two roles (Alice, Bob) of same agent; self-play training                                                                                                                                                            | Single agent with ensemble models (for disagreement)                                                                                                                                                     | Single agent with predictor network (for curiosity)                                                                                                                                              | Dual training of GAN (for goals) and agent on those goals                                                                                                                                                                         | Single agent SAC with ensemble critics (no second agent)                                                                                                                                                                                                                                |
| **Adversary/Generator‚Äôs Objective**  | Maximize `r_A = -Œª_v * V_P(s') + Œª_œÉ * CV_Q^+(s')` (value suppression + uncertainty exploitation)                                                                       | Maximize `r_A = -V_P(s')` (or scaled $- \hat{V}_P$)                                                                                                                         | Maximize protagonist‚Äôs cost (minimize its reward); effectively $r_A = -r_P$ (zero-sum)                                                      | Maximize protagonist‚Äôs ‚Äúrisk‚Äù (variance of return) and minimize its reward ‚Äì *risk-seeking adversary*                                                                                       | Maximize **regret** = (Protag‚Äôs loss - Antag‚Äôs loss) ‚Äì i.e., make env where protagonist does much worse than an expert                                                         | Make tasks that **Bob fails** but are achievable; internal reward for setting tricky but solvable tasks                                                                                                             | Maximize prediction **disagreement** among ensemble models (intrinsic reward signal)                                                                                                                     | Maximize **prediction error** of a forward model (intrinsic reward)                                                                                                                              | Generate goals of **intermediate difficulty** (via GAN: discriminator encourages 50% success rate goals)                                                                                                                          | N/A (no adversary, but ensemble aims to improve value estimates for robustness)                                                                                                                                                                                                         |
| **Protagonist/Agent‚Äôs Objective**    | Maximize environment reward (task completion), standard SAC with entropy                                                                                                | Maximize environment reward (task), standard SAC                                                                                                                            | Maximize task reward (e.g., forward locomotion) while adversary present (minimax)                                                           | Risk-averse objective: maximize reward, with penalty for variance (uses ensemble for value estimation)                                                                                      | Maximize task reward on environments generated by teacher (and for antagonist, same but competing)                                                                             | Bob: maximize internal reward by solving Alice‚Äôs task (return environment to initial state etc.)                                                                                                                    | Maximize *intrinsic reward* (exploration bonus); extrinsic not used in pure exploration phase                                                                                                            | Maximize *intrinsic curiosity reward* (with or without extrinsic reward)                                                                                                                         | Maximize reward of reaching GAN-generated goals (and eventually extrinsic goals)                                                                                                                                                  | Maximize extrinsic reward (same tasks as SAC) but with improved sample efficiency and stability                                                                                                                                                                                         |
| **Use of Uncertainty**               | ‚úÖ Yes ‚Äì **Q-ensemble (6 heads)** for $V_P$ and CV; adversary reward uses ensemble variance (epistemic uncertainty)                                                      | ‚ùå No ‚Äì single critic (or double) used directly, no uncertainty bonus                                                                                                        | ‚ùå No ‚Äì single model (various env seeds but no explicit uncertainty est.)                                                                    | ‚úÖ Yes ‚Äì **Q-ensemble (N heads)** to estimate variance of value; protagonist objective includes variance penalty, adversary exploits variance                                                | ‚úÖ Implicitly ‚Äì regret formulation means teacher tries env where protagonist uncertain enough to fail while antagonist succeeds (a proxy for protagonist‚Äôs uncertainty)         | ‚ùå No explicit uncertainty, but tasks naturally progress in difficulty                                                                                                                                               | ‚úÖ Yes ‚Äì **ensemble of dynamics models** measures disagreement (used as intrinsic reward for exploration)                                                                                                 | ‚úÖ Yes ‚Äì uses prediction error (uncertainty in dynamics prediction as curiosity)                                                                                                                  | ‚ùå Not explicitly (difficulty via success rate, not formal uncertainty)                                                                                                                                                            | ‚úÖ Yes ‚Äì **many critics** (usually 10) in ensemble; uses min/avg of critics for robust Q update and UCB for exploration                                                                                                                                                                  |
| **Normalization of Reward Signal**   | ‚úÖ Yes ‚Äì running **z-score** normalization for value estimates and CV (to keep rewards in stable range)                                                                  | ‚ùå No ‚Äì raw value function used directly (could be unbounded)                                                                                                                | ‚ùå No ‚Äì adversary reward = negative protagonist reward (bounded by task spec)                                                                | üî∏ Possibly scaled variance, but not explicitly z-normalized (not mentioned)                                                                                                                | üî∏ Regret is difference of rewards (bounded by score differences); no explicit normalization reported                                                                          | ‚ùå No ‚Äì internal reward is binary success/failure oriented                                                                                                                                                           | üî∏ Intrinsic rewards sometimes normalized or scaled in implementation, but conceptually raw disagreement used                                                                                            | üî∏ Often yes ‚Äì prediction error normalized (Burda used fixed scale or normalizing constants)                                                                                                     | ‚úÖ Yes ‚Äì GAN discriminator output inherently normalized to [0,1] as probability (goal difficulty measure)                                                                                                                          | ‚ùå No ‚Äì not applicable (extrinsic reward only, but target updates average errors)                                                                                                                                                                                                        |
| **Dynamic Curriculum or Scheduling** | ‚úÖ Yes ‚Äì **Œª_v and Œª_œÉ are scheduled** (Œª_v ramp-up, Œª_œÉ warmup then decay) to adjust focus over training                                                                | ‚ùå No ‚Äì static objective throughout training                                                                                                                                 | ‚ùå No ‚Äì adversary constantly maximizes protagonist cost; no curriculum except what emerges from training dynamics                            | üî∏ Partial ‚Äì risk aversion may be increased gradually; not a central point though                                                                                                           | üî∏ Partial ‚Äì the teacher automatically creates curriculum by learning (no explicit time schedule, but an emergent curriculum)                                                  | ‚úÖ Yes ‚Äì tasks get harder as Bob improves (Alice implicitly follows Bob‚Äôs capability; Alice gets reward only when Bob succeeds some and fails some)                                                                  | ‚úÖ Yes ‚Äì exploration will naturally shift as novel states become rare (intrinsic reward diminishes as environment is learned)                                                                             | üî∏ Yes ‚Äì in experiments, sometimes a coefficient is tuned; but curiosity usually constant or annealed if combined with extrinsic                                                                 | ‚úÖ Yes ‚Äì GAN difficulty tuning through discriminator ensures automatic curriculum (goals at 50% success level)                                                                                                                     | ‚ùå No ‚Äì ensemble use is static, though exploration bonus decays as uncertainty reduces (natural, not explicit schedule)                                                                                                                                                                  |
| **Environment/Test Domain**          | Continuous 2D maze navigation (custom Gymnasium), multi-maze evaluation (robustness across layouts)                                                                     | Similar continuous maze + a robot object disentangling task (IROS‚Äô20)                                                                                                       | MuJoCo locomotion (HalfCheetah, Ant, etc.) and classic control (InvertedPendulum)                                                           | TORCS driving simulator (autonomous driving scenarios with different dynamics)                                                                                                              | Partially observed 2D mazes (procedural), and a continuous control car racing task                                                                                             | Gridworld (MazeBase) tasks, and some continuous tasks (MountainCar, StarCraft sub-task) for evaluation                                                                                                              | Atari games (Monte Zuma‚Äôs Revenge etc.), MuJoCo tasks, and a real robotic arm pushing objects                                                                                                            | Many Atari games, VizDoom, etc. (54 environments in study)                                                                                                                                       | 2D navigation and simple manipulation tasks (e.g., reaching, pushing) ‚Äì tasks where a goal can be specified                                                                                                                       | MuJoCo continuous control (HalfCheetah, Hopper, Walker, Ant, etc.), focusing on sample efficiency and robustness                                                                                                                                                                        |
| **Results/Findings**                 | ‚Äì Improved protagonist success rate and robustness to unseen start states (expected, since it trains on worst-cases plus uncertainties). *[Our results TBD in thesis]*. | ‚Äì ARL protagonist outperformed non-ARL in generalizing to different initial states. Adversary automatically learned to set up challenging states (both sim and real robot). | ‚Äì Robust policies that could handle perturbations; outperformed domain randomization; adversary‚Äôs presence improved training stability too. | ‚Äì Protagonist learned safer driving policies; adversary uncovered edge cases (like slick patches) more efficiently than random perturbations. Ensemble-based variance helped quantify risk. | ‚Äì Protagonist achieved higher **zero-shot transfer** performance on unseen test envs compared to domain randomization. Emergent curriculum of increasing complexity in levels. | ‚Äì Agents trained with ASP learned much faster on downstream tasks than those without (due to having mastered environment dynamics). Showed that unsupervised self-play can replace a lot of task-specific training. | ‚Äì Demonstrated that maximizing model disagreement yields efficient exploration, succeeding in stochastic games where other methods failed. No extrinsic reward needed for learning meaningful behaviors. | ‚Äì Showed that pure curiosity can lead to decent performance on many games. Also identified that random feature targets (RND) work better than learned features in some cases for generalization. | ‚Äì Able to generate a sequence of goals that gradually stretched the agent‚Äôs ability (e.g., push object slightly further each time), outperforming naive curriculum. GAN avoided generating trivial or impossible goals by design. | ‚Äì Achieved **state-of-the-art sample efficiency** on continuous control; using 10 Q-networks and aggressive updates, results rivaled model-based methods. Validated that larger ensembles + UCB exploration can greatly speed up learning. Robustness to hyperparameters also improved. |
